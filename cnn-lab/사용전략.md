# AdamW Optimizer 파라미터 전략

## 1. 파라미터 정의

### Learning Rate (lr)
- **정의**: 각 학습 스텝에서 가중치를 얼마나 업데이트할지 결정하는 값
- **역할**:
  - 높으면 → 빠른 학습, 하지만 최적점을 지나칠 수 있음 (발산 위험)
  - 낮으면 → 안정적 학습, 하지만 느리고 local minima에 갇힐 수 있음
- **일반적 범위**: 1e-5 ~ 1e-3

### Weight Decay (weight-decay)
- **정의**: L2 정규화의 일종으로, 가중치가 너무 커지는 것을 방지
- **역할**:
  - 높으면 → 강한 정규화, 과적합 방지에 효과적
  - 낮으면 → 약한 정규화, 모델이 더 자유롭게 학습
- **AdamW 특징**: 기존 Adam과 달리 weight decay를 gradient와 분리하여 적용 (더 효과적인 정규화)
- **일반적 범위**: 1e-4 ~ 1e-1

---

## 2. 논문/실험 기반 권장값

### Deepfake Detection 관련 논문

| 논문/모델 | LR | Weight Decay | 비고 |
|-----------|-----|--------------|------|
| Xception (FaceForensics++) | 0.0002 | 0.0001 | 표준 설정 |
| EfficientNet-B4 | 0.001 | 0.01 | 큰 모델용 |
| ViT (Vision Transformer) | 0.0001 | 0.1 | Transformer 계열 |
| ResNet-50 fine-tuning | 0.0001 | 0.0001 | pretrained 기준 |

### ImageNet Fine-tuning 일반 권장값
```
LR: 1e-4 ~ 3e-4 (pretrained 모델 fine-tuning 시)
Weight Decay: 0.01 ~ 0.05
```

---

## 3. 소규모 데이터셋 (< 2만개) 전략

### 현재 상황 분석
- Real/Fake 각 1만개 미만 → 총 2만개 미만
- **과적합 위험 높음** → 강한 정규화 필요

### 권장 파라미터

```yaml
# 보수적 설정 (권장)
training:
  lr: 0.0001          # 낮은 LR로 안정적 학습
  weight-decay: 0.01  # 강한 정규화로 과적합 방지
  bs: 8
  early-stop: 20

# 또는 약간 공격적 설정
training:
  lr: 0.0003          # 조금 높은 LR
  weight-decay: 0.05  # 더 강한 정규화
  bs: 16
  early-stop: 15
```

### 현재 설정 vs 권장 설정 비교

| 파라미터 | 현재값 | 권장값 | 이유 |
|----------|--------|--------|------|
| lr | 0.001 | **0.0001~0.0003** | pretrained fine-tuning에 0.001은 다소 높음 |
| weight-decay | 0.001 | **0.01~0.05** | 데이터 적으면 더 강한 정규화 필요 |

---

## 4. 추가 권장 전략

### Learning Rate Scheduler
```python
# Cosine Annealing (권장)
scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
    optimizer, T_max=epochs, eta_min=1e-6
)

# 또는 ReduceLROnPlateau
scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
    optimizer, mode='min', factor=0.5, patience=5
)
```

### Warmup (선택사항)
- 처음 몇 epoch 동안 LR을 점진적으로 증가
- 소규모 데이터에서 안정성 향상

### 과적합 방지 추가 기법
1. **Data Augmentation** (augmentation.py 활용)
2. **Dropout** 증가 (0.3 → 0.5)
3. **Early Stopping** patience 조정
4. **Label Smoothing** (0.1 권장)

---

## 5. 최종 권장 설정

```yaml
# config/architecture.yaml
training:
  lr: 0.0002              # 기존 0.001 → 0.0002로 낮춤
  weight-decay: 0.01      # 기존 0.001 → 0.01로 높임
  bs: 8
  early-stop: 15

  # 추가 권장 (train.py에서 구현 필요)
  # scheduler: cosine
  # warmup-epochs: 3
  # label-smoothing: 0.1
```

---

## 참고 문헌
- "Decoupled Weight Decay Regularization" (Loshchilov & Hutter, 2019) - AdamW 논문
- "FaceForensics++: Learning to Detect Manipulated Facial Images" (Rossler et al., 2019)
- "EfficientNet: Rethinking Model Scaling for CNNs" (Tan & Le, 2019)
